{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Review Analysis using Naive Bayes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Objective: Our objective is to find the right alpha, feature Importance,Confusion Matrix, Precision, Recall and F1 score using Naive Bayes on Amazon Fine Food Review Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Loading the data\n",
    "\n",
    "I used SQLite to load the dataset as it is available in both 'csv' and 'SQLite Database'.\n",
    "I am using the SQLite 'final_half.sqlite' with time based slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geet/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using the SQLite Table to read data.\n",
    "con = sqlite3.connect('final_half.sqlite') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    153513\n",
      "negative     28573\n",
      "Name: Score, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sorted_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews_half\"\"\", con)\n",
    "labels_count = sorted_data['Score'].value_counts()\n",
    "labels = sorted_data['Score']\n",
    "print(labels_count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : We can clearly observe that it is an Imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182086, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>417839</td>\n",
       "      <td>451856</td>\n",
       "      <td>B00004CXX9</td>\n",
       "      <td>AIUWLEQ1ADEG5</td>\n",
       "      <td>Elizabeth Medina</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>944092800</td>\n",
       "      <td>Entertainingl Funny!</td>\n",
       "      <td>Beetlejuice is a well written movie ..... ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>346055</td>\n",
       "      <td>374359</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A344SMIA5JECGM</td>\n",
       "      <td>Vincent P. Ross</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>944438400</td>\n",
       "      <td>A modern day fairy tale</td>\n",
       "      <td>A twist of rumplestiskin captured on film, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>346041</td>\n",
       "      <td>374343</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>A1B2IZU1JLZA6</td>\n",
       "      <td>Wes</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>negative</td>\n",
       "      <td>948240000</td>\n",
       "      <td>WARNING: CLAMSHELL EDITION IS EDITED TV VERSION</td>\n",
       "      <td>I, myself always enjoyed this movie, it's very...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70688</td>\n",
       "      <td>76882</td>\n",
       "      <td>B00002N8SM</td>\n",
       "      <td>A32DW342WBJ6BX</td>\n",
       "      <td>Buttersugar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>948672000</td>\n",
       "      <td>A sure death for flies</td>\n",
       "      <td>I bought a few of these after my apartment was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346141</td>\n",
       "      <td>374450</td>\n",
       "      <td>B00004CI84</td>\n",
       "      <td>ACJR7EQF9S6FP</td>\n",
       "      <td>Jeremy Robertson</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>951523200</td>\n",
       "      <td>Bettlejuice...Bettlejuice...BETTLEJUICE!</td>\n",
       "      <td>What happens when you say his name three times...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index      Id   ProductId          UserId       ProfileName  \\\n",
       "0  417839  451856  B00004CXX9   AIUWLEQ1ADEG5  Elizabeth Medina   \n",
       "1  346055  374359  B00004CI84  A344SMIA5JECGM   Vincent P. Ross   \n",
       "2  346041  374343  B00004CI84   A1B2IZU1JLZA6               Wes   \n",
       "3   70688   76882  B00002N8SM  A32DW342WBJ6BX       Buttersugar   \n",
       "4  346141  374450  B00004CI84   ACJR7EQF9S6FP  Jeremy Robertson   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator     Score       Time  \\\n",
       "0                     0                       0  positive  944092800   \n",
       "1                     1                       2  positive  944438400   \n",
       "2                    19                      23  negative  948240000   \n",
       "3                     0                       0  positive  948672000   \n",
       "4                     2                       3  positive  951523200   \n",
       "\n",
       "                                           Summary  \\\n",
       "0                             Entertainingl Funny!   \n",
       "1                          A modern day fairy tale   \n",
       "2  WARNING: CLAMSHELL EDITION IS EDITED TV VERSION   \n",
       "3                           A sure death for flies   \n",
       "4         Bettlejuice...Bettlejuice...BETTLEJUICE!   \n",
       "\n",
       "                                                Text  \n",
       "0  Beetlejuice is a well written movie ..... ever...  \n",
       "1  A twist of rumplestiskin captured on film, sta...  \n",
       "2  I, myself always enjoyed this movie, it's very...  \n",
       "3  I bought a few of these after my apartment was...  \n",
       "4  What happens when you say his name three times...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data.head()         # Time is in ascending order which means the dataset is of Time Based slicing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "final_counts = count_vect.fit_transform(sorted_data['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182086, 82343)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "X_1, X_test, y_1, y_test = cross_validation.train_test_split(final_counts, labels, test_size=0.3, random_state=0)\n",
    "# split the train data set into cross validation train and cross validation test\n",
    "X_tr, X_cv, y_tr, y_cv = cross_validation.train_test_split(X_1, y_1, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList = list(range(1,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.86508930836408049, 0.85456495566019797, 0.84707809255023858, 0.84265083906963711, 0.84040926330852217, 0.84008421905099484, 0.8399497087527964, 0.8400618012957809, 0.84027475929029927, 0.84031961489957718]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "cv_scores = []\n",
    "for alpha in myList:\n",
    "    nb = BernoulliNB(alpha=alpha)\n",
    "    scores = cross_val_score(nb, X_tr, y_tr, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "print(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of alpha is 1.\n"
     ]
    }
   ],
   "source": [
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_aplha = myList[MSE.index(min(MSE))]\n",
    "print('\\nThe optimal number of alpha is %d.' % optimal_aplha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the NB classifier for k = 1 is 86.726101%\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model k = optimal_k\n",
    "NB_optimal = BernoulliNB(alpha=optimal_aplha)\n",
    "\n",
    "# fitting the model\n",
    "NB_optimal.fit(X_tr, y_tr)\n",
    "\n",
    "# predict the response\n",
    "pred = NB_optimal.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(y_test, pred) * 100\n",
    "print('\\nThe accuracy of the NB classifier for k = %d is %f%%' % (optimal_aplha, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41171 49363 49364 ..., 74648  5738 73802]\n",
      "[82342 35525 35526 ..., 74648  5738 73802]\n",
      "top negative words \n",
      " ['intermittently' 'mizudashi' 'mizuki' 'mj' 'mjr' 'mkae' 'mke' 'mkt'\n",
      " 'mla75001' 'mlfsd']\n",
      "top positive words \n",
      " ['Ã®t' 'goshoptnt' 'gosicknic' 'gospel' 'gossip' 'titanic' 'gota' 'gotchas'\n",
      " 'gottcha' 'tisk']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted = NB_optimal.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted = NB_optimal.feature_log_prob_[1, :].argsort()\n",
    "print(neg_class_prob_sorted)\n",
    "print(pos_class_prob_sorted)\n",
    "print('top negative words \\n',np.take(count_vect.get_feature_names(), neg_class_prob_sorted[:10]))\n",
    "print('top positive words \\n',np.take(count_vect.get_feature_names(), pos_class_prob_sorted[:10]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : Feature importance of the top 10 positive and negative words with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    46169\n",
      "negative     8457\n",
      "Name: Score, dtype: int64\n",
      "Counter({'positive': 48052, 'negative': 6574})\n",
      "54626\n"
     ]
    }
   ],
   "source": [
    "print(y_test.value_counts())\n",
    "import collections\n",
    "print(collections.Counter(pred))\n",
    "print(y_test.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3890  4567]\n",
      " [ 2684 43485]]\n",
      "tn - True Negavite Rate 3890\n",
      "fp - False Positive Rate 4567\n",
      "fn - False Negative Rate 2684\n",
      "tp - True Positive Rate 43485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels_conf = ['negative', 'positive']\n",
    "conf_mat = confusion_matrix(y_test, pred,labels = labels_conf)\n",
    "print(conf_mat)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "print('tn - True Negavite Rate',tn)\n",
    "print('fp - False Positive Rate',fp)\n",
    "print('fn - False Negative Rate',fn)\n",
    "print('tp - True Positive Rate',tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate 0.941865754077\n",
      "True Negative Rate 0.459973986047\n",
      "False Positive Rate 0.540026013953\n",
      "False Negative Rate 0.0581342459226\n"
     ]
    }
   ],
   "source": [
    "TPR = tp/(tp+fn)\n",
    "print('True Positive Rate',TPR)\n",
    "TNR = tn/(tn+fp)\n",
    "print('True Negative Rate',TNR)\n",
    "FPR = fp/(tn+fp)\n",
    "print('False Positive Rate',FPR)\n",
    "FNR = fn/(tp+fn)\n",
    "print('False Negative Rate',FNR)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : \n",
    " \n",
    " * TPR is high because as we observe that this dataset is Imbalanced and it is more inclined towards positive points.\n",
    " * TNR is low because the negative points are very less.\n",
    " * FPR is high as it was predicting the negative points to positive because of more positive points.\n",
    " * FNR is very low and clearly the model is not making any misclassfication in predicting positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score 0.904957129776\n",
      "recall_score 0.941865754077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('precision_score',precision_score(y_test, pred,pos_label='positive'))\n",
    "print('recall_score',recall_score(y_test, pred,pos_label='positive'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : \n",
    " * Precision : of all the points the model declared or predicted to be positive, 90% of them are actually positive.\n",
    " * Recall    : of all the points which actually belong to positive class, 94% of them are predicted to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925219863573\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, pred,pos_label='positive')\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : \n",
    "    * It combines both precision and recall. F1 score is high with 92%. The model is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182086, 1884182)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf = tf_idf_vect.fit_transform(sorted_data['Text'].values)\n",
    "print(final_tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data set into train and test\n",
    "X_1_tf, X_test_tf, y_1_tf, y_test_tf = cross_validation.train_test_split(final_tf_idf, labels, test_size=0.3, random_state=0)\n",
    "# split the train data set into cross validation train and cross validation test\n",
    "X_tr_tf, X_cv_tf, y_tr_tf, y_cv_tf = cross_validation.train_test_split(X_1_tf, y_1_tf, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993, 0.84213535130182993]\n"
     ]
    }
   ],
   "source": [
    "myList_tf = list(range(1,11))\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "cv_scores_tf_idf = []\n",
    "for alpha_tf in myList_tf:\n",
    "    nb_tf = MultinomialNB(alpha=alpha_tf)\n",
    "    scores_tf = cross_val_score(nb_tf, X_tr_tf, y_tr_tf, cv=10, scoring='accuracy')\n",
    "    cv_scores_tf_idf.append(scores_tf.mean())\n",
    "print(cv_scores_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The optimal number of alpha is 1.\n"
     ]
    }
   ],
   "source": [
    "# changing to misclassification error\n",
    "MSE_tf = [1 - x for x in cv_scores_tf_idf]\n",
    "\n",
    "# determining best k\n",
    "optimal_aplha_tf = myList_tf[MSE_tf.index(min(MSE_tf))]\n",
    "print('\\nThe optimal number of alpha is %d.' % optimal_aplha_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the NB classifier for k = 1 is 84.518361%\n"
     ]
    }
   ],
   "source": [
    "# instantiate learning model k = optimal_k\n",
    "NB_optimal_tf = MultinomialNB(alpha=optimal_aplha_tf)\n",
    "\n",
    "# fitting the model\n",
    "NB_optimal_tf.fit(X_tr_tf, y_tr_tf)\n",
    "\n",
    "# predict the response\n",
    "pred_tf = NB_optimal_tf.predict(X_test_tf)\n",
    "\n",
    "# evaluate accuracy\n",
    "acc_tf = accuracy_score(y_test_tf, pred_tf) * 100\n",
    "print('\\nThe accuracy of the NB classifier for k = %d is %f%%' % (optimal_aplha_tf, acc_tf))\n",
    "# print(NB_optimal.feature_log_prob_)\n",
    "# print(NB_optimal.class_count_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 942090 1189649 1189648 ...,   89472  860134 1620802]\n",
      "[ 942090 1016312 1016324 ...,  860134   89472 1620802]\n",
      "['liquid mothballs' 'overall meal' 'overall maybe' 'overall mass'\n",
      " 'overall make' 'overall maintains' 'overall made' 'overall low'\n",
      " 'overall loved' 'overall love']\n",
      "['liquid mothballs' 'mild boring' 'mild burning' 'mild burnt' 'mild butter'\n",
      " 'mild ca' 'mild candy' 'mild capuccino' 'mild category' 'mild chared']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted_tf = NB_optimal_tf.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted_tf = NB_optimal_tf.feature_log_prob_[1, :].argsort()\n",
    "print(neg_class_prob_sorted_tf)\n",
    "print(pos_class_prob_sorted_tf)\n",
    "print(np.take(tf_idf_vect.get_feature_names(), neg_class_prob_sorted_tf[:10]))\n",
    "print(np.take(tf_idf_vect.get_feature_names(), pos_class_prob_sorted_tf[:10]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : Feature importance of the top 10 positive and negative words with highest probability using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0  8457]\n",
      " [    0 46169]]\n",
      "tn - True Negative Rate 0\n",
      "fp - False Positive Rate 8457\n",
      "fn - False Negative Rate 0\n",
      "tp - True Positive Rate 46169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat_tf = confusion_matrix(y_test_tf, pred_tf)\n",
    "print(conf_mat_tf)\n",
    "tn_tf, fp_tf, fn_tf, tp_tf = confusion_matrix(y_test_tf, pred_tf).ravel()\n",
    "print('tn - True Negative Rate',tn_tf)\n",
    "print('fp - False Positive Rate',fp_tf)\n",
    "print('fn - False Negative Rate',fn_tf)\n",
    "print('tp - True Positive Rate',tp_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR - True Positive Rate 1.0\n",
      "TNR - True Negative Rate 0.0\n",
      "FPR - False Positive Rate 1.0\n",
      "FNR - False Negative Rate 0.0\n"
     ]
    }
   ],
   "source": [
    "TPR_tf = tp_tf/(tp_tf+fn_tf)\n",
    "print('TPR - True Positive Rate',TPR_tf)\n",
    "TNR_tf = tn_tf/(tn_tf+fp_tf)\n",
    "print('TNR - True Negative Rate',TNR_tf)\n",
    "FPR_tf = fp_tf/(tn_tf+fp_tf)\n",
    "print('FPR - False Positive Rate',FPR_tf)\n",
    "FNR_tf = fn_tf/(tp_tf+fn_tf)\n",
    "print('FNR - False Negative Rate',FNR_tf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : \n",
    " \n",
    " * TPR is high because as we observe that this dataset is Imbalanced and it is totally inclined towards positive points when using TF-IDF.\n",
    " * TNR is 0 because the negative points are very less and more positive points.\n",
    " * FPR is high as it was predicting the negative points to positive because of more positive points.\n",
    " * FNR is very low and clearly the model is not making any misclassfication in predicting positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score 0.845183612199\n",
      "recall_score 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "print('precision_score',precision_score(y_test_tf, pred_tf,pos_label='positive'))\n",
    "print('recall_score',recall_score(y_test_tf, pred_tf,pos_label='positive'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report : \n",
    " * Precision : of all the points the model declared or predicted to be positive, 84% of them are actually positive.\n",
    " * Recall    : of all the points which actually belong to positive class, 100% of them are predicted to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916097028622\n"
     ]
    }
   ],
   "source": [
    "f1_tf = f1_score(y_test_tf, pred_tf,pos_label='positive')\n",
    "print(f1_tf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Report : \n",
    "    * It combines both precision and recall. F1 score is high with 91%.\n",
    "    * TNR is 0 it means the model is not predicting well when we are using TF-IDF.\n",
    "    * TNR and TPR should be high for the model to perform well."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Report: Finally Bag of words technique is giving better results when compared to TF-IDF with an Imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
